{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.signal as signal\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import metrics\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score,  accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 메모리 설정 코드 추가\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADHD subjects: dict_keys(['v10p', 'v12p', 'v14p', 'v15p', 'v173', 'v177', 'v179', 'v181', 'v183', 'v18p', 'v190', 'v196', 'v198', 'v19p', 'v1p', 'v200', 'v204', 'v206', 'v209', 'v20p', 'v213', 'v215', 'v219', 'v21p', 'v227', 'v22p', 'v231', 'v234', 'v236', 'v238', 'v244', 'v246', 'v24p', 'v250', 'v254', 'v25p', 'v263', 'v265', 'v270', 'v274', 'v279', 'v27p', 'v284', 'v286', 'v288', 'v28p', 'v29p', 'v30p', 'v31p', 'v32p', 'v33p', 'v34p', 'v35p', 'v36p', 'v37p', 'v38p', 'v39p', 'v3p', 'v40p', 'v6p', 'v8p'])\n",
      "Control subjects: dict_keys(['v107', 'v108', 'v109', 'v110', 'v111', 'v112', 'v113', 'v114', 'v115', 'v116', 'v117', 'v118', 'v120', 'v121', 'v123', 'v125', 'v127', 'v129', 'v131', 'v133', 'v134', 'v138', 'v140', 'v143', 'v147', 'v149', 'v151', 'v297', 'v298', 'v299', 'v300', 'v302', 'v303', 'v304', 'v305', 'v306', 'v307', 'v308', 'v309', 'v310', 'v41p', 'v42p', 'v43p', 'v44p', 'v45p', 'v46p', 'v47p', 'v48p', 'v49p', 'v50p', 'v51p', 'v52p', 'v53p', 'v54p', 'v55p', 'v56p', 'v57p', 'v58p', 'v59p', 'v60p'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ADHD 및 Control 파일 디렉토리 설정\n",
    "adhd_save_dir = r\"C:\\Users\\gsubi\\OneDrive\\Desktop\\EEG_project\\ADHD_epochs\"\n",
    "control_save_dir = r\"C:\\Users\\gsubi\\OneDrive\\Desktop\\EEG_project\\Control_epochs\"\n",
    "\n",
    "# 파일명에서 피험자 ID와 에포크 번호를 추출하는 함수 (언더바 앞은 모두 피험자 ID로 설정)\n",
    "def extract_subject_epoch(file_name):\n",
    "    # 파일명을 먼저 확장자를 제외한 부분으로 나눔\n",
    "    base_name = os.path.splitext(file_name)[0]\n",
    "    \n",
    "    # 언더바('_')로 파일명을 나누어 피험자 ID와 에포크 번호를 추출\n",
    "    if '_' in base_name:\n",
    "        parts = base_name.split('_')\n",
    "        subject_id = parts[0]  # 언더바 앞의 부분이 피험자 ID\n",
    "        epoch_num = parts[1][1:]  # 'e' 다음의 숫자가 에포크 번호\n",
    "        return subject_id, epoch_num\n",
    "    return None, None\n",
    "\n",
    "# ADHD와 Control 디렉토리에서 파일 읽기\n",
    "adhd_files = os.listdir(adhd_save_dir)\n",
    "control_files = os.listdir(control_save_dir)\n",
    "\n",
    "adhd_subjects = {}\n",
    "control_subjects = {}\n",
    "\n",
    "# ADHD 파일들의 피험자 ID별로 그룹화\n",
    "for file in adhd_files:\n",
    "    subject_id, epoch_num = extract_subject_epoch(file)\n",
    "    if subject_id:\n",
    "        if subject_id not in adhd_subjects:\n",
    "            adhd_subjects[subject_id] = []\n",
    "        adhd_subjects[subject_id].append(os.path.join(adhd_save_dir, file))\n",
    "\n",
    "# Control 파일들의 피험자 ID별로 그룹화\n",
    "for file in control_files:\n",
    "    subject_id, epoch_num = extract_subject_epoch(file)\n",
    "    if subject_id:\n",
    "        if subject_id not in control_subjects:\n",
    "            control_subjects[subject_id] = []\n",
    "        control_subjects[subject_id].append(os.path.join(control_save_dir, file))\n",
    "\n",
    "print(f\"ADHD subjects: {adhd_subjects.keys()}\")\n",
    "print(f\"Control subjects: {control_subjects.keys()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # 첫 번째 Residual Block\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same', input_shape=input_shape, name='conv1'))\n",
    "    model.add(layers.BatchNormalization(name='batch_norm1'))\n",
    "    model.add(layers.ReLU(name='relu1'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), name='maxpool1'))\n",
    "    model.add(layers.Dropout(0.3, name='dropout1'))\n",
    "\n",
    "    # 두 번째 Residual Block\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same', name='conv2'))\n",
    "    model.add(layers.BatchNormalization(name='batch_norm2'))\n",
    "    model.add(layers.ReLU(name='relu2'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), name='maxpool2'))\n",
    "    model.add(layers.Dropout(0.4, name='dropout2'))\n",
    "\n",
    "    # 세 번째 Residual Block\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same', name='conv3'))\n",
    "    model.add(layers.BatchNormalization(name='batch_norm3'))\n",
    "    model.add(layers.ReLU(name='relu3'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), name='maxpool3'))\n",
    "    model.add(layers.Dropout(0.5, name='dropout3'))\n",
    "\n",
    "    # 네 번째 Residual Block\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same', name='conv4'))\n",
    "    model.add(layers.BatchNormalization(name='batch_norm4'))\n",
    "    model.add(layers.ReLU(name='relu4'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), name='maxpool4'))\n",
    "    model.add(layers.Dropout(0.5, name='dropout4'))\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    model.add(layers.Flatten(name='flatten'))\n",
    "    model.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001), name='fc1'))\n",
    "    model.add(layers.Dropout(0.5, name='dropout5'))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid', name='output'))\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# 학습률 스케줄러 (ReduceLROnPlateau)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
    "\n",
    "# Early Stopping (과적합 방지)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Model Checkpoint (최적의 가중치 저장)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADHD와 Control 피험자 분리\n",
    "adhd_subjects_list = list(adhd_subjects.keys())\n",
    "control_subjects_list = list(control_subjects.keys())\n",
    "\n",
    "# ADHD는 1, Control은 0으로 라벨 설정\n",
    "all_subjects = adhd_subjects_list + control_subjects_list\n",
    "all_labels = [1] * len(adhd_subjects_list) + [0] * len(control_subjects_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    # 훈련 및 검증 손실 시각화\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss 그래프\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy 그래프\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_tsne(model, data, labels):\n",
    "    # 모델의 중간 레이어에서 출력된 임베딩을 얻기 위해 flatten 레이어 전까지 모델을 자름\n",
    "    layer_outputs = models.Model(inputs=model.input, outputs=model.get_layer('flatten').output)\n",
    "    \n",
    "    # 학습된 모델을 통해 데이터 임베딩 추출\n",
    "    embeddings = layer_outputs.predict(data)\n",
    "    \n",
    "    # t-SNE 적용 (n_components=2로 2차원 공간에 시각화)\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    reduced_data = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # ADHD와 Control 데이터 구분하여 시각화\n",
    "    adhd_idx = (labels == 1)\n",
    "    control_idx = (labels == 0)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(reduced_data[adhd_idx, 0], reduced_data[adhd_idx, 1], color='red', label='ADHD', alpha=0.5)\n",
    "    plt.scatter(reduced_data[control_idx, 0], reduced_data[control_idx, 1], color='blue', label='Control', alpha=0.5)\n",
    "    \n",
    "    # x축과 y축 레이블 추가\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "\n",
    "    plt.title('t-SNE visualization of data embeddings')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플링 주파수 설정 (128Hz)\n",
    "fs = 128\n",
    "\n",
    "# 데이터를 불러오고 각 채널별로 PSD를 계산하는 함수\n",
    "def load_data_with_psd(file_paths):\n",
    "    data = []\n",
    "    for file_path in file_paths:\n",
    "        mat = scipy.io.loadmat(file_path)\n",
    "        \n",
    "        # 파일명에서 확장자를 제외한 기본 파일명 추출\n",
    "        base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        \n",
    "        # 기본 파일명을 그대로 키로 사용하여 데이터 로드\n",
    "        if base_name in mat:\n",
    "            raw_data = mat[base_name]\n",
    "            \n",
    "            num_channels = raw_data.shape[1]  # 19개 채널 (열 개수)\n",
    "            psd_data = []\n",
    "            \n",
    "            # 각 채널별로 PSD 계산\n",
    "            for channel in range(num_channels):\n",
    "                channel_data = raw_data[:, channel]  # 각 채널 데이터 (2560 샘플)\n",
    "                freqs, psd = signal.welch(channel_data, fs=fs, nperseg=128)  # nperseg을 샘플 길이에 맞게 설정\n",
    "                psd_data.append(psd)\n",
    "            \n",
    "            psd_data = np.array(psd_data)\n",
    "            \n",
    "            # psd 데이터를 CNN이 처리할 수 있는 형태로 변환 (예: 64x65 크기로 재구성)\n",
    "            if psd_data.shape[1] >= 64:  # 주파수 대역이 64 이상일 때만\n",
    "                psd_reshaped = psd_data[:, :64].reshape(19, 64, 1)  # 채널 수를 유지한 채 19x64x1 크기로 변환\n",
    "                data.append(psd_reshaped)\n",
    "            else:\n",
    "                raise ValueError(f\"PSD data is too small for {base_name}. Expected at least 64 frequency bins.\")\n",
    "        else:\n",
    "            raise KeyError(f\"'{base_name}' key not found in {file_path}\")\n",
    "    \n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 지표 저장 리스트\n",
    "train_accuracy_list = []\n",
    "accuracy_list = []\n",
    "precision_adhd_list = []\n",
    "precision_control_list = []\n",
    "recall_adhd_list = []\n",
    "recall_control_list = []\n",
    "f1_adhd_list = []\n",
    "f1_control_list = []\n",
    "auc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of all_data: 778\n",
      "Length of all_labels: 778\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(all_data, all_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 입력 데이터의 형태에 따라 input_shape를 설정합니다.\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m \u001b[43mall_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (19, 64, 1) 형태일 것으로 예상됩니다.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# CNN 모델 생성 및 학습\u001b[39;00m\n\u001b[0;32m     32\u001b[0m cnn_model \u001b[38;5;241m=\u001b[39m create_cnn_model(input_shape)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "# 초기화\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "# 각 ADHD 피험자에 대해 PSD 데이터를 로드하고 all_data와 all_labels에 추가\n",
    "for subject in adhd_subjects.keys():\n",
    "    data = load_data_with_psd(adhd_subjects[subject])\n",
    "    all_data.extend(data)\n",
    "    all_labels.extend([1] * len(data))  # ADHD는 1로 라벨링\n",
    "\n",
    "# 각 Control 피험자에 대해 PSD 데이터를 로드하고 all_data와 all_labels에 추가\n",
    "for subject in control_subjects.keys():\n",
    "    data = load_data_with_psd(control_subjects[subject])\n",
    "    all_data.extend(data)\n",
    "    all_labels.extend([0] * len(data))  # Control은 0으로 라벨링\n",
    "\n",
    "# 데이터를 NumPy 배열로 변환\n",
    "all_data = np.array(all_data)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# 데이터의 길이 확인\n",
    "print(f\"Length of all_data: {len(all_data)}\")\n",
    "print(f\"Length of all_labels: {len(all_labels)}\")\n",
    "\n",
    "# 학습 및 테스트 데이터 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(all_data, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 입력 데이터의 형태에 따라 input_shape를 설정합니다.\n",
    "input_shape = all_data.shape[1:]  # (19, 64, 1) 형태일 것으로 예상됩니다.\n",
    "\n",
    "# CNN 모델 생성 및 학습\n",
    "cnn_model = create_cnn_model(input_shape)\n",
    "history = cnn_model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "# 모델 평가\n",
    "train_metrics = cnn_model.evaluate(x_train, y_train, verbose=1)\n",
    "test_metrics = cnn_model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "# 평가 결과 추출\n",
    "train_loss, train_acc = train_metrics[0], train_metrics[1]\n",
    "test_loss, test_acc = test_metrics[0], test_metrics[1]\n",
    "\n",
    "# 예측 결과 가져오기\n",
    "y_pred = cnn_model.predict(x_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "# Confusion Matrix 기반의 Precision, Recall, F1-Score 계산\n",
    "test_precision_adhd = precision_score(y_test, y_pred_classes, pos_label=1)\n",
    "test_precision_control = precision_score(y_test, y_pred_classes, pos_label=0)\n",
    "test_recall_adhd = recall_score(y_test, y_pred_classes, pos_label=1)\n",
    "test_recall_control = recall_score(y_test, y_pred_classes, pos_label=0)\n",
    "test_f1_adhd = f1_score(y_test, y_pred_classes, pos_label=1)\n",
    "test_f1_control = f1_score(y_test, y_pred_classes, pos_label=0)\n",
    "\n",
    "# AUC 계산\n",
    "test_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# 성능 결과 출력\n",
    "print(\"Model Performance without Cross-Validation\")\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "print(f\"Test Precision (ADHD): {test_precision_adhd}, Control: {test_precision_control}\")\n",
    "print(f\"Test Recall (ADHD): {test_recall_adhd}, Control: {test_recall_control}\")\n",
    "print(f\"Test F1-Score (ADHD): {test_f1_adhd}, Control: {test_f1_control}\")\n",
    "print(f\"Test AUC: {test_auc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
